# DPO_Pair
## Code function:
Train DPO model with: dpo_train.py  
Train sft model with: sft_train.py  
sample data with: sample_ultra.py  
compute reward with: reward_batch.py

## Data
[sample data and reward for llama3-base sft mode](https://huggingface.co/datasets/YaoYX/llama_sft_sample)   
[sample data and reward for mistral-base sft mode](https://huggingface.co/datasets/YaoYX/mistral_sft_sample)  
[sample data and reward for llama3-instruct mode](https://huggingface.co/datasets/YaoYX/llama_instruct_sample)  
[sample data and reward for mistral-instruct mode](https://huggingface.co/datasets/YaoYX/mistral_instruct_sample)  
## Some CKs
 ### mistral instruct 
 #### ($\max, \min$)  
 [huggingface link](https://huggingface.co/YaoYX/Mistral_instruct_chosen_mu_1000sigma_rejected_mu_-1000sigma)  

